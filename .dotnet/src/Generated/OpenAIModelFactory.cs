// <auto-generated/>

using System;
using System.Collections.Generic;
using System.Linq;

namespace OpenAI.Internal.Models
{
    /// <summary> Model factory for models. </summary>
    internal static partial class OpenAIModelFactory
    {
        /// <summary> Initializes a new instance of <see cref="Models.CompletionUsage"/>. </summary>
        /// <param name="promptTokens"> Number of tokens in the prompt. </param>
        /// <param name="completionTokens"> Number of tokens in the generated completion. </param>
        /// <param name="totalTokens"> Total number of tokens used in the request (prompt + completion). </param>
        /// <returns> A new <see cref="Models.CompletionUsage"/> instance for mocking. </returns>
        public static CompletionUsage CompletionUsage(long promptTokens = default, long completionTokens = default, long totalTokens = default)
        {
            return new CompletionUsage(promptTokens, completionTokens, totalTokens, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateCompletionRequest"/>. </summary>
        /// <param name="model">
        /// ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to
        /// see all of your available models, or see our [Model overview](/docs/models/overview) for
        /// descriptions of them.
        /// </param>
        /// <param name="prompt">
        /// The prompt(s) to generate completions for, encoded as a string, array of strings, array of
        /// tokens, or array of token arrays.
        ///
        /// Note that &lt;|endoftext|&gt; is the document separator that the model sees during training, so if a
        /// prompt is not specified the model will generate as if from the beginning of a new document.
        /// </param>
        /// <param name="bestOf">
        /// Generates `best_of` completions server-side and returns the "best" (the one with the highest
        /// log probability per token). ClientResults cannot be streamed.
        ///
        /// When used with `n`, `best_of` controls the number of candidate completions and `n` specifies
        /// how many to return â€“ `best_of` must be greater than `n`.
        ///
        /// **Note:** Because this parameter generates many completions, it can quickly consume your token
        /// quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
        /// </param>
        /// <param name="echo"> Echo back the prompt in addition to the completion. </param>
        /// <param name="frequencyPenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
        /// frequency in the text so far, decreasing the model's likelihood to repeat the same line
        /// verbatim.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
        /// </param>
        /// <param name="logitBias">
        /// Modify the likelihood of specified tokens appearing in the completion.
        ///
        /// Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an
        /// associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe)
        /// to convert text to token IDs. Mathematically, the bias is added to the logits generated by the
        /// model prior to sampling. The exact effect will vary per model, but values between -1 and 1
        /// should decrease or increase likelihood of selection; values like -100 or 100 should result in a
        /// ban or exclusive selection of the relevant token.
        ///
        /// As an example, you can pass `{"50256": -100}` to prevent the &lt;|endoftext|&gt; token from being
        /// generated.
        /// </param>
        /// <param name="logprobs">
        /// Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens.
        /// For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The
        /// API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1`
        /// elements in the response.
        ///
        /// The maximum value for `logprobs` is 5.
        /// </param>
        /// <param name="maxTokens">
        /// The maximum number of [tokens](/tokenizer) to generate in the completion.
        ///
        /// The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        /// [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
        /// for counting tokens.
        /// </param>
        /// <param name="n">
        /// How many completions to generate for each prompt.
        ///
        /// **Note:** Because this parameter generates many completions, it can quickly consume your token
        /// quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
        /// </param>
        /// <param name="presencePenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
        /// in the text so far, increasing the model's likelihood to talk about new topics.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
        /// </param>
        /// <param name="seed">
        /// If specified, our system will make a best effort to sample deterministically, such that
        /// repeated requests with the same `seed` and parameters should return the same result.
        ///
        /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response
        /// parameter to monitor changes in the backend.
        /// </param>
        /// <param name="stop"> Up to 4 sequences where the API will stop generating further tokens. </param>
        /// <param name="stream">
        /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
        /// [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
        /// as they become available, with the stream terminated by a `data: [DONE]` message.
        /// [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
        /// </param>
        /// <param name="suffix"> The suffix that comes after a completion of inserted text. </param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
        /// more random, while lower values like 0.2 will make it more focused and deterministic.
        ///
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers
        /// the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
        /// the top 10% probability mass are considered.
        ///
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="user">
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect
        /// abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
        /// </param>
        /// <returns> A new <see cref="Models.CreateCompletionRequest"/> instance for mocking. </returns>
        public static CreateCompletionRequest CreateCompletionRequest(CreateCompletionRequestModel model = default, BinaryData prompt = null, long? bestOf = null, bool? echo = null, double? frequencyPenalty = null, IDictionary<string, long> logitBias = null, long? logprobs = null, long? maxTokens = null, long? n = null, double? presencePenalty = null, long? seed = null, BinaryData stop = null, bool? stream = null, string suffix = null, double? temperature = null, double? topP = null, string user = null)
        {
            logitBias ??= new Dictionary<string, long>();

            return new CreateCompletionRequest(
                model,
                prompt,
                bestOf,
                echo,
                frequencyPenalty,
                logitBias,
                logprobs,
                maxTokens,
                n,
                presencePenalty,
                seed,
                stop,
                stream,
                suffix,
                temperature,
                topP,
                user,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateCompletionResponse"/>. </summary>
        /// <param name="id"> A unique identifier for the completion. </param>
        /// <param name="choices"> The list of completion choices the model generated for the input. </param>
        /// <param name="created"> The Unix timestamp (in seconds) of when the completion was created. </param>
        /// <param name="model"> The model used for the completion. </param>
        /// <param name="systemFingerprint">
        /// This fingerprint represents the backend configuration that the model runs with.
        ///
        /// Can be used in conjunction with the `seed` request parameter to understand when backend changes
        /// have been made that might impact determinism.
        /// </param>
        /// <param name="object"> The object type, which is always `text_completion`. </param>
        /// <param name="usage"> Usage statistics for the completion request. </param>
        /// <returns> A new <see cref="Models.CreateCompletionResponse"/> instance for mocking. </returns>
        public static CreateCompletionResponse CreateCompletionResponse(string id = null, IEnumerable<CreateCompletionResponseChoice> choices = null, DateTimeOffset created = default, string model = null, string systemFingerprint = null, CreateCompletionResponseObject @object = default, CompletionUsage usage = null)
        {
            choices ??= new List<CreateCompletionResponseChoice>();

            return new CreateCompletionResponse(
                id,
                choices?.ToList(),
                created,
                model,
                systemFingerprint,
                @object,
                usage,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateCompletionResponseChoice"/>. </summary>
        /// <param name="index"></param>
        /// <param name="text"></param>
        /// <param name="logprobs"></param>
        /// <param name="finishReason">
        /// The reason the model stopped generating tokens. This will be `stop` if the model hit a
        /// natural stop point or a provided stop sequence, or `content_filter` if content was omitted
        /// due to a flag from our content filters, `length` if the maximum number of tokens specified
        /// in the request was reached, or `content_filter` if content was omitted due to a flag from our
        /// content filters.
        /// </param>
        /// <returns> A new <see cref="Models.CreateCompletionResponseChoice"/> instance for mocking. </returns>
        public static CreateCompletionResponseChoice CreateCompletionResponseChoice(long index = default, string text = null, CreateCompletionResponseChoiceLogprobs logprobs = null, CreateCompletionResponseChoiceFinishReason finishReason = default)
        {
            return new CreateCompletionResponseChoice(index, text, logprobs, finishReason, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateCompletionResponseChoiceLogprobs"/>. </summary>
        /// <param name="tokens"></param>
        /// <param name="tokenLogprobs"></param>
        /// <param name="topLogprobs"></param>
        /// <param name="textOffset"></param>
        /// <returns> A new <see cref="Models.CreateCompletionResponseChoiceLogprobs"/> instance for mocking. </returns>
        public static CreateCompletionResponseChoiceLogprobs CreateCompletionResponseChoiceLogprobs(IEnumerable<string> tokens = null, IEnumerable<double> tokenLogprobs = null, IEnumerable<IDictionary<string, long>> topLogprobs = null, IEnumerable<long> textOffset = null)
        {
            tokens ??= new List<string>();
            tokenLogprobs ??= new List<double>();
            topLogprobs ??= new List<IDictionary<string, long>>();
            textOffset ??= new List<long>();

            return new CreateCompletionResponseChoiceLogprobs(tokens?.ToList(), tokenLogprobs?.ToList(), topLogprobs?.ToList(), textOffset?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateFineTuningJobRequest"/>. </summary>
        /// <param name="model">
        /// The name of the model to fine-tune. You can select one of the
        /// [supported models](/docs/guides/fine-tuning/what-models-can-be-fine-tuned).
        /// </param>
        /// <param name="trainingFile">
        /// The ID of an uploaded file that contains training data.
        ///
        /// See [upload file](/docs/api-reference/files/upload) for how to upload a file.
        ///
        /// Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with
        /// the purpose `fine-tune`.
        ///
        /// See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
        /// </param>
        /// <param name="hyperparameters"> The hyperparameters used for the fine-tuning job. </param>
        /// <param name="suffix">
        /// A string of up to 18 characters that will be added to your fine-tuned model name.
        ///
        /// For example, a `suffix` of "custom-model-name" would produce a model name like
        /// `ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel`.
        /// </param>
        /// <param name="validationFile">
        /// The ID of an uploaded file that contains validation data.
        ///
        /// If you provide this file, the data is used to generate validation metrics periodically during
        /// fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should
        /// not be present in both train and validation files.
        ///
        /// Your dataset must be formatted as a JSONL file. You must upload your file with the purpose
        /// `fine-tune`.
        ///
        /// See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
        /// </param>
        /// <returns> A new <see cref="Models.CreateFineTuningJobRequest"/> instance for mocking. </returns>
        public static CreateFineTuningJobRequest CreateFineTuningJobRequest(CreateFineTuningJobRequestModel model = default, string trainingFile = null, CreateFineTuningJobRequestHyperparameters hyperparameters = null, string suffix = null, string validationFile = null)
        {
            return new CreateFineTuningJobRequest(
                model,
                trainingFile,
                hyperparameters,
                suffix,
                validationFile,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.FineTuningJob"/>. </summary>
        /// <param name="id"> The object identifier, which can be referenced in the API endpoints. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the fine-tuning job was created. </param>
        /// <param name="error">
        /// For fine-tuning jobs that have `failed`, this will contain more information on the cause of the
        /// failure.
        /// </param>
        /// <param name="fineTunedModel">
        /// The name of the fine-tuned model that is being created. The value will be null if the
        /// fine-tuning job is still running.
        /// </param>
        /// <param name="finishedAt">
        /// The Unix timestamp (in seconds) for when the fine-tuning job was finished. The value will be
        /// null if the fine-tuning job is still running.
        /// </param>
        /// <param name="hyperparameters">
        /// The hyperparameters used for the fine-tuning job. See the
        /// [fine-tuning guide](/docs/guides/fine-tuning) for more details.
        /// </param>
        /// <param name="model"> The base model that is being fine-tuned. </param>
        /// <param name="object"> The object type, which is always "fine_tuning.job". </param>
        /// <param name="organizationId"> The organization that owns the fine-tuning job. </param>
        /// <param name="resultFiles">
        /// The compiled results file ID(s) for the fine-tuning job. You can retrieve the results with the
        /// [Files API](/docs/api-reference/files/retrieve-contents).
        /// </param>
        /// <param name="status">
        /// The current status of the fine-tuning job, which can be either `validating_files`, `queued`,
        /// `running`, `succeeded`, `failed`, or `cancelled`.
        /// </param>
        /// <param name="trainedTokens">
        /// The total number of billable tokens processed by this fine-tuning job. The value will be null
        /// if the fine-tuning job is still running.
        /// </param>
        /// <param name="trainingFile">
        /// The file ID used for training. You can retrieve the training data with the
        /// [Files API](/docs/api-reference/files/retrieve-contents).
        /// </param>
        /// <param name="validationFile">
        /// The file ID used for validation. You can retrieve the validation results with the
        /// [Files API](/docs/api-reference/files/retrieve-contents).
        /// </param>
        /// <returns> A new <see cref="Models.FineTuningJob"/> instance for mocking. </returns>
        public static FineTuningJob FineTuningJob(string id = null, DateTimeOffset createdAt = default, FineTuningJobError error = null, string fineTunedModel = null, DateTimeOffset? finishedAt = null, FineTuningJobHyperparameters hyperparameters = null, string model = null, FineTuningJobObject @object = default, string organizationId = null, IEnumerable<string> resultFiles = null, FineTuningJobStatus status = default, long? trainedTokens = null, string trainingFile = null, string validationFile = null)
        {
            resultFiles ??= new List<string>();

            return new FineTuningJob(
                id,
                createdAt,
                error,
                fineTunedModel,
                finishedAt,
                hyperparameters,
                model,
                @object,
                organizationId,
                resultFiles?.ToList(),
                status,
                trainedTokens,
                trainingFile,
                validationFile,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.FineTuningJobError"/>. </summary>
        /// <param name="code"> A machine-readable error code. </param>
        /// <param name="message"> A human-readable error message. </param>
        /// <param name="param">
        /// The parameter that was invalid, usually `training_file` or `validation_file`. This field will
        /// be null if the failure was not parameter-specific.
        /// </param>
        /// <returns> A new <see cref="Models.FineTuningJobError"/> instance for mocking. </returns>
        public static FineTuningJobError FineTuningJobError(string code = null, string message = null, string param = null)
        {
            return new FineTuningJobError(code, message, param, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.FineTuningJobHyperparameters"/>. </summary>
        /// <param name="nEpochs">
        /// The number of epochs to train the model for. An epoch refers to one full cycle through the
        /// training dataset.
        ///
        /// "auto" decides the optimal number of epochs based on the size of the dataset. If setting the
        /// number manually, we support any number between 1 and 50 epochs.
        /// </param>
        /// <returns> A new <see cref="Models.FineTuningJobHyperparameters"/> instance for mocking. </returns>
        public static FineTuningJobHyperparameters FineTuningJobHyperparameters(BinaryData nEpochs = null)
        {
            return new FineTuningJobHyperparameters(nEpochs, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListPaginatedFineTuningJobsResponse"/>. </summary>
        /// <param name="data"></param>
        /// <param name="hasMore"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.ListPaginatedFineTuningJobsResponse"/> instance for mocking. </returns>
        public static ListPaginatedFineTuningJobsResponse ListPaginatedFineTuningJobsResponse(IEnumerable<FineTuningJob> data = null, bool hasMore = default, ListPaginatedFineTuningJobsResponseObject @object = default)
        {
            data ??= new List<FineTuningJob>();

            return new ListPaginatedFineTuningJobsResponse(data?.ToList(), hasMore, @object, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListFineTuningJobEventsResponse"/>. </summary>
        /// <param name="data"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.ListFineTuningJobEventsResponse"/> instance for mocking. </returns>
        public static ListFineTuningJobEventsResponse ListFineTuningJobEventsResponse(IEnumerable<FineTuningJobEvent> data = null, ListFineTuningJobEventsResponseObject @object = default)
        {
            data ??= new List<FineTuningJobEvent>();

            return new ListFineTuningJobEventsResponse(data?.ToList(), @object, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.FineTuningJobEvent"/>. </summary>
        /// <param name="id"></param>
        /// <param name="createdAt"></param>
        /// <param name="level"></param>
        /// <param name="message"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.FineTuningJobEvent"/> instance for mocking. </returns>
        public static FineTuningJobEvent FineTuningJobEvent(string id = null, DateTimeOffset createdAt = default, FineTuningJobEventLevel level = default, string message = null, FineTuningJobEventObject @object = default)
        {
            return new FineTuningJobEvent(
                id,
                createdAt,
                level,
                message,
                @object,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateMessageRequest"/>. </summary>
        /// <param name="role"> The role of the entity that is creating the message. Currently only `user` is supported. </param>
        /// <param name="content"> The content of the message. </param>
        /// <param name="fileIds">
        /// A list of [File](/docs/api-reference/files) IDs that the message should use. There can be a
        /// maximum of 10 files attached to a message. Useful for tools like `retrieval` and
        /// `code_interpreter` that can access and use files.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.CreateMessageRequest"/> instance for mocking. </returns>
        public static CreateMessageRequest CreateMessageRequest(CreateMessageRequestRole role = default, string content = null, IEnumerable<string> fileIds = null, IDictionary<string, string> metadata = null)
        {
            fileIds ??= new List<string>();
            metadata ??= new Dictionary<string, string>();

            return new CreateMessageRequest(role, content, fileIds?.ToList(), metadata, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.MessageObject"/>. </summary>
        /// <param name="id"> The identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread.message`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the message was created. </param>
        /// <param name="threadId"> The [thread](/docs/api-reference/threads) ID that this message belongs to. </param>
        /// <param name="status"> The status of the message, which can be either in_progress, incomplete, or completed. </param>
        /// <param name="incompleteDetails"> On an incomplete message, details about why the message is incomplete. </param>
        /// <param name="completedAt"> The Unix timestamp at which the message was completed. </param>
        /// <param name="incompleteAt"> The Unix timestamp at which the message was marked as incomplete. </param>
        /// <param name="role"> The entity that produced the message. One of `user` or `assistant`. </param>
        /// <param name="content"> The content of the message in array of text and/or images. </param>
        /// <param name="assistantId">
        /// If applicable, the ID of the [assistant](/docs/api-reference/assistants) that authored this
        /// message.
        /// </param>
        /// <param name="runId">
        /// If applicable, the ID of the [run](/docs/api-reference/runs) associated with the authoring of
        /// this message.
        /// </param>
        /// <param name="fileIds">
        /// A list of [file](/docs/api-reference/files) IDs that the assistant should use. Useful for
        /// tools like retrieval and code_interpreter that can access files. A maximum of 10 files can be
        /// attached to a message.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.MessageObject"/> instance for mocking. </returns>
        public static MessageObject MessageObject(string id = null, MessageObjectObject @object = default, DateTimeOffset createdAt = default, string threadId = null, MessageObjectStatus status = default, MessageObjectIncompleteDetails incompleteDetails = null, DateTimeOffset? completedAt = null, DateTimeOffset? incompleteAt = null, MessageObjectRole role = default, IEnumerable<BinaryData> content = null, string assistantId = null, string runId = null, IEnumerable<string> fileIds = null, IReadOnlyDictionary<string, string> metadata = null)
        {
            content ??= new List<BinaryData>();
            fileIds ??= new List<string>();
            metadata ??= new Dictionary<string, string>();

            return new MessageObject(
                id,
                @object,
                createdAt,
                threadId,
                status,
                incompleteDetails,
                completedAt,
                incompleteAt,
                role,
                content?.ToList(),
                assistantId,
                runId,
                fileIds?.ToList(),
                metadata,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.MessageObjectIncompleteDetails"/>. </summary>
        /// <param name="reason"> The reason the message is incomplete. </param>
        /// <returns> A new <see cref="Models.MessageObjectIncompleteDetails"/> instance for mocking. </returns>
        public static MessageObjectIncompleteDetails MessageObjectIncompleteDetails(string reason = null)
        {
            return new MessageObjectIncompleteDetails(reason, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListMessagesResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListMessagesResponse"/> instance for mocking. </returns>
        public static ListMessagesResponse ListMessagesResponse(ListMessagesResponseObject @object = default, IEnumerable<MessageObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<MessageObject>();

            return new ListMessagesResponse(
                @object,
                data?.ToList(),
                firstId,
                lastId,
                hasMore,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListMessageFilesResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListMessageFilesResponse"/> instance for mocking. </returns>
        public static ListMessageFilesResponse ListMessageFilesResponse(ListMessageFilesResponseObject @object = default, IEnumerable<MessageFileObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<MessageFileObject>();

            return new ListMessageFilesResponse(
                @object,
                data?.ToList(),
                firstId,
                lastId,
                hasMore,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.MessageFileObject"/>. </summary>
        /// <param name="id"> TThe identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread.message.file`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the message file was created. </param>
        /// <param name="messageId"> The ID of the [message](/docs/api-reference/messages) that the [File](/docs/api-reference/files) is attached to. </param>
        /// <returns> A new <see cref="Models.MessageFileObject"/> instance for mocking. </returns>
        public static MessageFileObject MessageFileObject(string id = null, MessageFileObjectObject @object = default, DateTimeOffset createdAt = default, string messageId = null)
        {
            return new MessageFileObject(id, @object, createdAt, messageId, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListModelsResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <returns> A new <see cref="Models.ListModelsResponse"/> instance for mocking. </returns>
        public static ListModelsResponse ListModelsResponse(ListModelsResponseObject @object = default, IEnumerable<Model> data = null)
        {
            data ??= new List<Model>();

            return new ListModelsResponse(@object, data?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.Model"/>. </summary>
        /// <param name="id"> The model identifier, which can be referenced in the API endpoints. </param>
        /// <param name="created"> The Unix timestamp (in seconds) when the model was created. </param>
        /// <param name="object"> The object type, which is always "model". </param>
        /// <param name="ownedBy"> The organization that owns the model. </param>
        /// <returns> A new <see cref="Models.Model"/> instance for mocking. </returns>
        public static Model Model(string id = null, DateTimeOffset created = default, ModelObject @object = default, string ownedBy = null)
        {
            return new Model(id, created, @object, ownedBy, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.DeleteModelResponse"/>. </summary>
        /// <param name="id"></param>
        /// <param name="deleted"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.DeleteModelResponse"/> instance for mocking. </returns>
        public static DeleteModelResponse DeleteModelResponse(string id = null, bool deleted = default, DeleteModelResponseObject @object = default)
        {
            return new DeleteModelResponse(id, deleted, @object, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateThreadAndRunRequest"/>. </summary>
        /// <param name="assistantId"> The ID of the [assistant](/docs/api-reference/assistants) to use to execute this run. </param>
        /// <param name="thread"> If no thread is provided, an empty thread will be created. </param>
        /// <param name="model">
        /// The ID of the [Model](/docs/api-reference/models) to be used to execute this run. If a value is
        /// provided here, it will override the model associated with the assistant. If not, the model
        /// associated with the assistant will be used.
        /// </param>
        /// <param name="instructions">
        /// Override the default system message of the assistant. This is useful for modifying the behavior
        /// on a per-run basis.
        /// </param>
        /// <param name="tools">
        /// Override the tools the assistant can use for this run. This is useful for modifying the
        /// behavior on a per-run basis.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <param name="stream">
        /// If true, returns a stream of events that happen during the Run as server-sent events,
        /// terminating when the Run enters a terminal state with a data: [DONE] message.
        /// </param>
        /// <returns> A new <see cref="Models.CreateThreadAndRunRequest"/> instance for mocking. </returns>
        public static CreateThreadAndRunRequest CreateThreadAndRunRequest(string assistantId = null, CreateThreadRequest thread = null, string model = null, string instructions = null, IEnumerable<BinaryData> tools = null, IDictionary<string, string> metadata = null, bool? stream = null)
        {
            tools ??= new List<BinaryData>();
            metadata ??= new Dictionary<string, string>();

            return new CreateThreadAndRunRequest(
                assistantId,
                thread,
                model,
                instructions,
                tools?.ToList(),
                metadata,
                stream,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunObject"/>. </summary>
        /// <param name="id"> The identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread.run`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the run was created. </param>
        /// <param name="threadId">
        /// The ID of the [thread](/docs/api-reference/threads) that was executed on as a part of this
        /// run.
        /// </param>
        /// <param name="assistantId"> The ID of the [assistant](/docs/api-reference/assistants) used for execution of this run. </param>
        /// <param name="status">
        /// The status of the run, which can be either `queued`, `in_progress`, `requires_action`,
        /// `cancelling`, `cancelled`, `failed`, `completed`, or `expired`.
        /// </param>
        /// <param name="requiredAction">
        /// Details on the action required to continue the run. Will be `null` if no action is
        /// required.
        /// </param>
        /// <param name="lastError"> The last error associated with this run. Will be `null` if there are no errors. </param>
        /// <param name="expiresAt"> The Unix timestamp (in seconds) for when the run will expire. </param>
        /// <param name="startedAt"> The Unix timestamp (in seconds) for when the run was started. </param>
        /// <param name="cancelledAt"> The Unix timestamp (in seconds) for when the run was cancelled. </param>
        /// <param name="failedAt"> The Unix timestamp (in seconds) for when the run failed. </param>
        /// <param name="completedAt"> The Unix timestamp (in seconds) for when the run was completed. </param>
        /// <param name="model"> The model that the [assistant](/docs/api-reference/assistants) used for this run. </param>
        /// <param name="instructions"> The instructions that the [assistant](/docs/api-reference/assistants) used for this run. </param>
        /// <param name="tools"> The list of tools that the [assistant](/docs/api-reference/assistants) used for this run. </param>
        /// <param name="fileIds">
        /// The list of [File](/docs/api-reference/files) IDs the
        /// [assistant](/docs/api-reference/assistants) used for this run.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <param name="usage"></param>
        /// <returns> A new <see cref="Models.RunObject"/> instance for mocking. </returns>
        public static RunObject RunObject(string id = null, RunObjectObject @object = default, DateTimeOffset createdAt = default, string threadId = null, string assistantId = null, RunObjectStatus status = default, RunObjectRequiredAction requiredAction = null, RunObjectLastError lastError = null, DateTimeOffset? expiresAt = null, DateTimeOffset? startedAt = null, DateTimeOffset? cancelledAt = null, DateTimeOffset? failedAt = null, DateTimeOffset? completedAt = null, string model = null, string instructions = null, IEnumerable<BinaryData> tools = null, IEnumerable<string> fileIds = null, IReadOnlyDictionary<string, string> metadata = null, RunCompletionUsage usage = null)
        {
            tools ??= new List<BinaryData>();
            fileIds ??= new List<string>();
            metadata ??= new Dictionary<string, string>();

            return new RunObject(
                id,
                @object,
                createdAt,
                threadId,
                assistantId,
                status,
                requiredAction,
                lastError,
                expiresAt,
                startedAt,
                cancelledAt,
                failedAt,
                completedAt,
                model,
                instructions,
                tools?.ToList(),
                fileIds?.ToList(),
                metadata,
                usage,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunObjectRequiredAction"/>. </summary>
        /// <param name="type"> For now, this is always `submit_tool_outputs`. </param>
        /// <param name="submitToolOutputs"> Details on the tool outputs needed for this run to continue. </param>
        /// <returns> A new <see cref="Models.RunObjectRequiredAction"/> instance for mocking. </returns>
        public static RunObjectRequiredAction RunObjectRequiredAction(RunObjectRequiredActionType type = default, RunObjectRequiredActionSubmitToolOutputs submitToolOutputs = null)
        {
            return new RunObjectRequiredAction(type, submitToolOutputs, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunObjectRequiredActionSubmitToolOutputs"/>. </summary>
        /// <param name="toolCalls"> A list of the relevant tool calls. </param>
        /// <returns> A new <see cref="Models.RunObjectRequiredActionSubmitToolOutputs"/> instance for mocking. </returns>
        public static RunObjectRequiredActionSubmitToolOutputs RunObjectRequiredActionSubmitToolOutputs(IEnumerable<RunToolCallObject> toolCalls = null)
        {
            toolCalls ??= new List<RunToolCallObject>();

            return new RunObjectRequiredActionSubmitToolOutputs(toolCalls?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunToolCallObject"/>. </summary>
        /// <param name="id">
        /// The ID of the tool call. This ID must be referenced when you submit the tool outputs in using
        /// the [Submit tool outputs to run](/docs/api-reference/runs/submitToolOutputs) endpoint.
        /// </param>
        /// <param name="type"> The type of tool call the output is required for. For now, this is always `function`. </param>
        /// <param name="function"> The function definition. </param>
        /// <returns> A new <see cref="Models.RunToolCallObject"/> instance for mocking. </returns>
        public static RunToolCallObject RunToolCallObject(string id = null, RunToolCallObjectType type = default, RunToolCallObjectFunction function = null)
        {
            return new RunToolCallObject(id, type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunToolCallObjectFunction"/>. </summary>
        /// <param name="name"> The name of the function. </param>
        /// <param name="arguments"> The arguments that the model expects you to pass to the function. </param>
        /// <returns> A new <see cref="Models.RunToolCallObjectFunction"/> instance for mocking. </returns>
        public static RunToolCallObjectFunction RunToolCallObjectFunction(string name = null, string arguments = null)
        {
            return new RunToolCallObjectFunction(name, arguments, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunObjectLastError"/>. </summary>
        /// <param name="code"> One of `server_error`, `rate_limit_exceeded`, or `invalid_prompt`. </param>
        /// <param name="message"> A human-readable description of the error. </param>
        /// <returns> A new <see cref="Models.RunObjectLastError"/> instance for mocking. </returns>
        public static RunObjectLastError RunObjectLastError(RunObjectLastErrorCode code = default, string message = null)
        {
            return new RunObjectLastError(code, message, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunCompletionUsage"/>. </summary>
        /// <param name="completionTokens"> Number of completion tokens used over the course of the run. </param>
        /// <param name="promptTokens"> Number of prompt tokens used over the course of the run. </param>
        /// <param name="totalTokens"> Total number of tokens used (prompt + completion). </param>
        /// <returns> A new <see cref="Models.RunCompletionUsage"/> instance for mocking. </returns>
        public static RunCompletionUsage RunCompletionUsage(long completionTokens = default, long promptTokens = default, long totalTokens = default)
        {
            return new RunCompletionUsage(completionTokens, promptTokens, totalTokens, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateRunRequest"/>. </summary>
        /// <param name="assistantId"> The ID of the [assistant](/docs/api-reference/assistants) to use to execute this run. </param>
        /// <param name="model">
        /// The ID of the [Model](/docs/api-reference/models) to be used to execute this run. If a value
        /// is provided here, it will override the model associated with the assistant. If not, the model
        /// associated with the assistant will be used.
        /// </param>
        /// <param name="instructions">
        /// Overrides the [instructions](/docs/api-reference/assistants/createAssistant) of the assistant.
        /// This is useful for modifying the behavior on a per-run basis.
        /// </param>
        /// <param name="additionalInstructions">
        /// Appends additional instructions at the end of the instructions for the run. This is useful for
        /// modifying the behavior on a per-run basis without overriding other instructions.
        /// </param>
        /// <param name="tools">
        /// Override the tools the assistant can use for this run. This is useful for modifying the
        /// behavior on a per-run basis.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <param name="stream">
        /// If true, returns a stream of events that happen during the Run as server-sent events,
        /// terminating when the Run enters a terminal state with a data: [DONE] message.
        /// </param>
        /// <returns> A new <see cref="Models.CreateRunRequest"/> instance for mocking. </returns>
        public static CreateRunRequest CreateRunRequest(string assistantId = null, string model = null, string instructions = null, string additionalInstructions = null, IEnumerable<BinaryData> tools = null, IDictionary<string, string> metadata = null, bool? stream = null)
        {
            tools ??= new List<BinaryData>();
            metadata ??= new Dictionary<string, string>();

            return new CreateRunRequest(
                assistantId,
                model,
                instructions,
                additionalInstructions,
                tools?.ToList(),
                metadata,
                stream,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListRunsResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListRunsResponse"/> instance for mocking. </returns>
        public static ListRunsResponse ListRunsResponse(ListRunsResponseObject @object = default, IEnumerable<RunObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<RunObject>();

            return new ListRunsResponse(
                @object,
                data?.ToList(),
                firstId,
                lastId,
                hasMore,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListRunStepsResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListRunStepsResponse"/> instance for mocking. </returns>
        public static ListRunStepsResponse ListRunStepsResponse(ListRunStepsResponseObject @object = default, IEnumerable<RunStepObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<RunStepObject>();

            return new ListRunStepsResponse(
                @object,
                data?.ToList(),
                firstId,
                lastId,
                hasMore,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunStepObject"/>. </summary>
        /// <param name="id"> The identifier of the run step, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread.run.step`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the run step was created. </param>
        /// <param name="assistantId"> The ID of the [assistant](/docs/api-reference/assistants) associated with the run step. </param>
        /// <param name="threadId"> The ID of the [thread](/docs/api-reference/threads) that was run. </param>
        /// <param name="runId"> The ID of the [run](/docs/api-reference/runs) that this run step is a part of. </param>
        /// <param name="type"> The type of run step, which can be either `message_creation` or `tool_calls`. </param>
        /// <param name="status">
        /// The status of the run step, which can be either `in_progress`, `cancelled`, `failed`,
        /// `completed`, or `expired`.
        /// </param>
        /// <param name="stepDetails"> The details of the run step. </param>
        /// <param name="lastError"> The last error associated with this run step. Will be `null` if there are no errors. </param>
        /// <param name="expiresAt">
        /// The Unix timestamp (in seconds) for when the run step expired. A step is considered expired
        /// if the parent run is expired.
        /// </param>
        /// <param name="cancelledAt"> The Unix timestamp (in seconds) for when the run step was cancelled. </param>
        /// <param name="failedAt"> The Unix timestamp (in seconds) for when the run step failed. </param>
        /// <param name="completedAt"> T The Unix timestamp (in seconds) for when the run step completed.. </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <param name="usage"></param>
        /// <returns> A new <see cref="Models.RunStepObject"/> instance for mocking. </returns>
        public static RunStepObject RunStepObject(string id = null, RunStepObjectObject @object = default, DateTimeOffset createdAt = default, string assistantId = null, string threadId = null, string runId = null, RunStepObjectType type = default, RunStepObjectStatus status = default, BinaryData stepDetails = null, RunStepObjectLastError lastError = null, DateTimeOffset? expiresAt = null, DateTimeOffset? cancelledAt = null, DateTimeOffset? failedAt = null, DateTimeOffset? completedAt = null, IReadOnlyDictionary<string, string> metadata = null, RunCompletionUsage usage = null)
        {
            metadata ??= new Dictionary<string, string>();

            return new RunStepObject(
                id,
                @object,
                createdAt,
                assistantId,
                threadId,
                runId,
                type,
                status,
                stepDetails,
                lastError,
                expiresAt,
                cancelledAt,
                failedAt,
                completedAt,
                metadata,
                usage,
                serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunStepObjectLastError"/>. </summary>
        /// <param name="code"> One of `server_error` or `rate_limit_exceeded`. </param>
        /// <param name="message"> A human-readable description of the error. </param>
        /// <returns> A new <see cref="Models.RunStepObjectLastError"/> instance for mocking. </returns>
        public static RunStepObjectLastError RunStepObjectLastError(RunStepObjectLastErrorCode code = default, string message = null)
        {
            return new RunStepObjectLastError(code, message, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ThreadObject"/>. </summary>
        /// <param name="id"> The identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the thread was created. </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.ThreadObject"/> instance for mocking. </returns>
        public static ThreadObject ThreadObject(string id = null, ThreadObjectObject @object = default, DateTimeOffset createdAt = default, IReadOnlyDictionary<string, string> metadata = null)
        {
            metadata ??= new Dictionary<string, string>();

            return new ThreadObject(id, @object, createdAt, metadata, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.DeleteThreadResponse"/>. </summary>
        /// <param name="id"></param>
        /// <param name="deleted"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.DeleteThreadResponse"/> instance for mocking. </returns>
        public static DeleteThreadResponse DeleteThreadResponse(string id = null, bool deleted = default, DeleteThreadResponseObject @object = default)
        {
            return new DeleteThreadResponse(id, deleted, @object, serializedAdditionalRawData: null);
        }
    }
}
